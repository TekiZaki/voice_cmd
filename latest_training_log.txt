PS C:\Dzaki\Tugas\UMB\Sem 7\Pengantar Deep Learning\voice_cmd> py model.py
2026-01-23 17:40:06.286409: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-23 17:40:10.750383: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
============================================================
ğŸ¤ VOICE COMMAND MODEL TRAINER
============================================================

ğŸ“¦ Loading dataset...
ğŸ“‚ Detected classes: ['atas', 'backspace', 'back_home', 'bawah', 'buka_mc-edge', 'buka_note_win_10', 'buka_wa', 'close', 'copy', 'force close', 'hello_voicecmd', 'kanan', 'kiri', 'new_tab', 'paste', 'percepat', 'perlambat', 'refresh', 'searching', 'select_all', 'sleep_cmd', 'tab', 'tab_down', 'tab_up']
   atas: 40 samples
   backspace: 60 samples
   back_home: 60 samples
   bawah: 40 samples
   buka_mc-edge: 60 samples
   buka_note_win_10: 50 samples
   buka_wa: 50 samples
   close: 50 samples
   copy: 40 samples
   force close: 50 samples
   hello_voicecmd: 60 samples
   kanan: 40 samples
   kiri: 40 samples
   new_tab: 40 samples
   paste: 40 samples
   percepat: 40 samples
   perlambat: 40 samples
   refresh: 40 samples
   searching: 40 samples
   select_all: 40 samples
   sleep_cmd: 60 samples
   tab: 30 samples
   tab_down: 70 samples
   tab_up: 70 samples

ğŸ“Š Sample Analysis:
   atas: 40 samples âœ…
   backspace: 60 samples âœ…
   back_home: 60 samples âœ…
   bawah: 40 samples âœ…
   buka_mc-edge: 60 samples âœ…
   buka_note_win_10: 50 samples âœ…
   buka_wa: 50 samples âœ…
   close: 50 samples âœ…
   copy: 40 samples âœ…
   force close: 50 samples âœ…
   hello_voicecmd: 60 samples âœ…
   kanan: 40 samples âœ…
   kiri: 40 samples âœ…
   new_tab: 40 samples âœ…
   paste: 40 samples âœ…
   percepat: 40 samples âœ…
   perlambat: 40 samples âœ…
   refresh: 40 samples âœ…
   searching: 40 samples âœ…
   select_all: 40 samples âœ…
   sleep_cmd: 60 samples âœ…
   tab: 30 samples âœ…
   tab_down: 70 samples âœ…
   tab_up: 70 samples âœ…

ğŸ”Š Generating synthetic 'background' noise...
   Generated 47 background samples

âœ… Total samples loaded: 1197
ğŸ“ Number of classes: 25
ğŸ“ Classes: [np.str_('atas'), np.str_('back_home'), np.str_('background'), np.str_('backspace'), np.str_('bawah'), np.str_('buka_mc-edge'), np.str_('buka_note_win_10'), np.str_('buka_wa'), np.str_('close'), np.str_('copy'), np.str_('force close'), np.str_('hello_voicecmd'), np.str_('kanan'), np.str_('kiri'), np.str_('new_tab'), np.str_('paste'), np.str_('percepat'), np.str_('perlambat'), np.str_('refresh'), np.str_('searching'), np.str_('select_all'), np.str_('sleep_cmd'), np.str_('tab'), np.str_('tab_down'), np.str_('tab_up')]

ğŸ“Š Class Distribution:
   atas: 40 samples
   back_home: 60 samples
   background: 47 samples
   backspace: 60 samples
   bawah: 40 samples
   buka_mc-edge: 60 samples
   buka_note_win_10: 50 samples
   buka_wa: 50 samples
   close: 50 samples
   copy: 40 samples
   force close: 50 samples
   hello_voicecmd: 60 samples
   kanan: 40 samples
   kiri: 40 samples
   new_tab: 40 samples
   paste: 40 samples
   percepat: 40 samples
   perlambat: 40 samples
   refresh: 40 samples
   searching: 40 samples
   select_all: 40 samples
   sleep_cmd: 60 samples
   tab: 30 samples
   tab_down: 70 samples
   tab_up: 70 samples

ğŸ’¾ Label encoder saved

ğŸ”€ Data split:
   Training samples: 957
   Testing samples: 240

ğŸ—ï¸  Input shape: (87, 120)
2026-01-23 17:41:12.616236: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "sequential"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                         â”ƒ Output Shape                â”ƒ         Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ reshape (Reshape)                    â”‚ (None, 87, 120, 1)          â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d (Conv2D)                      â”‚ (None, 87, 120, 32)         â”‚             320 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalization                  â”‚ (None, 87, 120, 32)         â”‚             128 â”‚
â”‚ (BatchNormalization)                 â”‚                             â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d (MaxPooling2D)         â”‚ (None, 43, 60, 32)          â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout (Dropout)                    â”‚ (None, 43, 60, 32)          â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_1 (Conv2D)                    â”‚ (None, 43, 60, 64)          â”‚          18,496 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalization_1                â”‚ (None, 43, 60, 64)          â”‚             256 â”‚
â”‚ (BatchNormalization)                 â”‚                             â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d_1 (MaxPooling2D)       â”‚ (None, 21, 30, 64)          â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_1 (Dropout)                  â”‚ (None, 21, 30, 64)          â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_2 (Conv2D)                    â”‚ (None, 21, 30, 128)         â”‚          73,856 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalization_2                â”‚ (None, 21, 30, 128)         â”‚             512 â”‚
â”‚ (BatchNormalization)                 â”‚                             â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d_2 (MaxPooling2D)       â”‚ (None, 10, 15, 128)         â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_2 (Dropout)                  â”‚ (None, 10, 15, 128)         â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten (Flatten)                    â”‚ (None, 19200)               â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense (Dense)                        â”‚ (None, 128)                 â”‚       2,457,728 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batch_normalization_3                â”‚ (None, 128)                 â”‚             512 â”‚
â”‚ (BatchNormalization)                 â”‚                             â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_3 (Dropout)                  â”‚ (None, 128)                 â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_1 (Dense)                      â”‚ (None, 64)                  â”‚           8,256 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_4 (Dropout)                  â”‚ (None, 64)                  â”‚               0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_2 (Dense)                      â”‚ (None, 25)                  â”‚           1,625 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,561,689 (9.77 MB)
 Trainable params: 2,560,985 (9.77 MB)
 Non-trainable params: 704 (2.75 KB)

ğŸš€ Training model...
============================================================
Epoch 1/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17s 235ms/step - accuracy: 0.0726 - loss: 3.8688 - val_accuracy: 0.1292 - val_loss: 3.0057 - learning_rate: 0.0010
Epoch 2/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 234ms/step - accuracy: 0.1691 - loss: 2.8460 - val_accuracy: 0.4167 - val_loss: 2.3816 - learning_rate: 0.0010
Epoch 3/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 221ms/step - accuracy: 0.2812 - loss: 2.5487 - val_accuracy: 0.2292 - val_loss: 2.3908 - learning_rate: 0.0010
Epoch 4/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 231ms/step - accuracy: 0.3676 - loss: 2.2946 - val_accuracy: 0.5542 - val_loss: 1.7599 - learning_rate: 0.0010
Epoch 5/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 226ms/step - accuracy: 0.4152 - loss: 2.0840 - val_accuracy: 0.4875 - val_loss: 1.7742 - learning_rate: 0.0010
Epoch 6/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 224ms/step - accuracy: 0.4523 - loss: 1.8404 - val_accuracy: 0.6750 - val_loss: 1.3499 - learning_rate: 0.0010
Epoch 7/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 225ms/step - accuracy: 0.5259 - loss: 1.6044 - val_accuracy: 0.6917 - val_loss: 1.3321 - learning_rate: 0.0010
Epoch 8/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 233ms/step - accuracy: 0.5528 - loss: 1.5005 - val_accuracy: 0.7875 - val_loss: 1.0256 - learning_rate: 0.0010
Epoch 9/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 220ms/step - accuracy: 0.5848 - loss: 1.4262 - val_accuracy: 0.7833 - val_loss: 1.0231 - learning_rate: 0.0010
Epoch 10/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 222ms/step - accuracy: 0.6634 - loss: 1.1878 - val_accuracy: 0.7875 - val_loss: 0.9306 - learning_rate: 0.0010
Epoch 11/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 225ms/step - accuracy: 0.6735 - loss: 1.1557 - val_accuracy: 0.8292 - val_loss: 0.8498 - learning_rate: 0.0010
Epoch 12/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 226ms/step - accuracy: 0.7431 - loss: 0.9802 - val_accuracy: 0.8917 - val_loss: 0.5250 - learning_rate: 0.0010
Epoch 13/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 223ms/step - accuracy: 0.6951 - loss: 0.9650 - val_accuracy: 0.8875 - val_loss: 0.6512 - learning_rate: 0.0010
Epoch 14/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 223ms/step - accuracy: 0.7166 - loss: 0.8818 - val_accuracy: 0.9042 - val_loss: 0.5377 - learning_rate: 0.0010
Epoch 15/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 230ms/step - accuracy: 0.7439 - loss: 0.9163 - val_accuracy: 0.8958 - val_loss: 0.4645 - learning_rate: 0.0010
Epoch 16/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 234ms/step - accuracy: 0.7460 - loss: 0.7914 - val_accuracy: 0.9333 - val_loss: 0.3363 - learning_rate: 0.0010
Epoch 17/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 231ms/step - accuracy: 0.8207 - loss: 0.6741 - val_accuracy: 0.9500 - val_loss: 0.2941 - learning_rate: 0.0010
Epoch 18/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 227ms/step - accuracy: 0.8074 - loss: 0.6598 - val_accuracy: 0.9042 - val_loss: 0.4186 - learning_rate: 0.0010
Epoch 19/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 230ms/step - accuracy: 0.8263 - loss: 0.5992 - val_accuracy: 0.9375 - val_loss: 0.2942 - learning_rate: 0.0010
Epoch 20/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 230ms/step - accuracy: 0.8628 - loss: 0.5110 - val_accuracy: 0.9292 - val_loss: 0.2829 - learning_rate: 0.0010
Epoch 21/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 231ms/step - accuracy: 0.8395 - loss: 0.5006 - val_accuracy: 0.9333 - val_loss: 0.3186 - learning_rate: 0.0010
Epoch 22/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 225ms/step - accuracy: 0.8371 - loss: 0.5441 - val_accuracy: 0.9458 - val_loss: 0.2394 - learning_rate: 0.0010
Epoch 23/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 233ms/step - accuracy: 0.8484 - loss: 0.5512 - val_accuracy: 0.9542 - val_loss: 0.2605 - learning_rate: 0.0010
Epoch 24/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 232ms/step - accuracy: 0.8812 - loss: 0.4303 - val_accuracy: 0.9458 - val_loss: 0.2533 - learning_rate: 0.0010
Epoch 25/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 225ms/step - accuracy: 0.8612 - loss: 0.4382 - val_accuracy: 0.9542 - val_loss: 0.2205 - learning_rate: 0.0010
Epoch 26/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 224ms/step - accuracy: 0.8600 - loss: 0.4690 - val_accuracy: 0.9542 - val_loss: 0.2474 - learning_rate: 0.0010
Epoch 27/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 235ms/step - accuracy: 0.8622 - loss: 0.4016 - val_accuracy: 0.9417 - val_loss: 0.3029 - learning_rate: 0.0010
Epoch 28/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 236ms/step - accuracy: 0.8794 - loss: 0.4116 - val_accuracy: 0.9583 - val_loss: 0.2087 - learning_rate: 0.0010
Epoch 29/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 220ms/step - accuracy: 0.8954 - loss: 0.3449 - val_accuracy: 0.9500 - val_loss: 0.2303 - learning_rate: 0.0010
Epoch 30/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 239ms/step - accuracy: 0.8801 - loss: 0.3773 - val_accuracy: 0.9625 - val_loss: 0.2178 - learning_rate: 0.0010
Epoch 31/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 233ms/step - accuracy: 0.8917 - loss: 0.3594 - val_accuracy: 0.9583 - val_loss: 0.2127 - learning_rate: 0.0010
Epoch 32/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 232ms/step - accuracy: 0.8654 - loss: 0.4216 - val_accuracy: 0.9708 - val_loss: 0.1963 - learning_rate: 0.0010
Epoch 33/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 226ms/step - accuracy: 0.8940 - loss: 0.3727 - val_accuracy: 0.9417 - val_loss: 0.2262 - learning_rate: 0.0010
Epoch 34/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 223ms/step - accuracy: 0.8995 - loss: 0.3292 - val_accuracy: 0.9500 - val_loss: 0.2182 - learning_rate: 0.0010
Epoch 35/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s 258ms/step - accuracy: 0.9111 - loss: 0.2834 - val_accuracy: 0.9667 - val_loss: 0.1738 - learning_rate: 0.0010
Epoch 36/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 229ms/step - accuracy: 0.9234 - loss: 0.2565 - val_accuracy: 0.9625 - val_loss: 0.2064 - learning_rate: 0.0010
Epoch 37/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 235ms/step - accuracy: 0.9079 - loss: 0.2814 - val_accuracy: 0.9667 - val_loss: 0.1697 - learning_rate: 0.0010
Epoch 38/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 230ms/step - accuracy: 0.9256 - loss: 0.2478 - val_accuracy: 0.9667 - val_loss: 0.1819 - learning_rate: 0.0010
Epoch 39/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 227ms/step - accuracy: 0.9066 - loss: 0.2778 - val_accuracy: 0.9667 - val_loss: 0.1905 - learning_rate: 0.0010
Epoch 40/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 233ms/step - accuracy: 0.9282 - loss: 0.2444 - val_accuracy: 0.9583 - val_loss: 0.2005 - learning_rate: 0.0010
Epoch 41/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 227ms/step - accuracy: 0.9248 - loss: 0.2417 - val_accuracy: 0.9583 - val_loss: 0.2109 - learning_rate: 0.0010
Epoch 42/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 230ms/step - accuracy: 0.9155 - loss: 0.2936
Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 239ms/step - accuracy: 0.9155 - loss: 0.2932 - val_accuracy: 0.9667 - val_loss: 0.2110 - learning_rate: 0.0010
Epoch 43/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 232ms/step - accuracy: 0.9185 - loss: 0.2694 - val_accuracy: 0.9708 - val_loss: 0.1719 - learning_rate: 5.0000e-04
Epoch 44/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 224ms/step - accuracy: 0.9107 - loss: 0.2382 - val_accuracy: 0.9625 - val_loss: 0.1886 - learning_rate: 5.0000e-04
Epoch 45/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 233ms/step - accuracy: 0.9330 - loss: 0.2171 - val_accuracy: 0.9708 - val_loss: 0.1760 - learning_rate: 5.0000e-04
Epoch 46/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 230ms/step - accuracy: 0.9064 - loss: 0.2476 - val_accuracy: 0.9667 - val_loss: 0.1941 - learning_rate: 5.0000e-04
Epoch 47/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 224ms/step - accuracy: 0.9362 - loss: 0.2063
Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 232ms/step - accuracy: 0.9362 - loss: 0.2063 - val_accuracy: 0.9667 - val_loss: 0.1903 - learning_rate: 5.0000e-04
Epoch 48/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 231ms/step - accuracy: 0.9345 - loss: 0.1986 - val_accuracy: 0.9667 - val_loss: 0.1768 - learning_rate: 2.5000e-04
Epoch 49/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 224ms/step - accuracy: 0.9306 - loss: 0.2113 - val_accuracy: 0.9708 - val_loss: 0.1723 - learning_rate: 2.5000e-04
Epoch 50/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s 244ms/step - accuracy: 0.9401 - loss: 0.1899 - val_accuracy: 0.9708 - val_loss: 0.1659 - learning_rate: 2.5000e-04
Epoch 51/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 237ms/step - accuracy: 0.9613 - loss: 0.1528 - val_accuracy: 0.9750 - val_loss: 0.1720 - learning_rate: 2.5000e-04
Epoch 52/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s 243ms/step - accuracy: 0.9552 - loss: 0.1702 - val_accuracy: 0.9750 - val_loss: 0.1725 - learning_rate: 2.5000e-04
Epoch 53/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 237ms/step - accuracy: 0.9370 - loss: 0.1973 - val_accuracy: 0.9750 - val_loss: 0.1800 - learning_rate: 2.5000e-04
Epoch 54/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 239ms/step - accuracy: 0.9516 - loss: 0.1473 - val_accuracy: 0.9750 - val_loss: 0.1690 - learning_rate: 2.5000e-04
Epoch 55/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 216ms/step - accuracy: 0.9656 - loss: 0.1111
Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 225ms/step - accuracy: 0.9655 - loss: 0.1114 - val_accuracy: 0.9750 - val_loss: 0.1662 - learning_rate: 2.5000e-04
Epoch 56/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 229ms/step - accuracy: 0.9514 - loss: 0.1586 - val_accuracy: 0.9750 - val_loss: 0.1664 - learning_rate: 1.2500e-04
Epoch 57/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 225ms/step - accuracy: 0.9687 - loss: 0.1374 - val_accuracy: 0.9708 - val_loss: 0.1698 - learning_rate: 1.2500e-04
Epoch 58/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 225ms/step - accuracy: 0.9533 - loss: 0.1668 - val_accuracy: 0.9708 - val_loss: 0.1666 - learning_rate: 1.2500e-04
Epoch 59/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 233ms/step - accuracy: 0.9577 - loss: 0.1381 - val_accuracy: 0.9750 - val_loss: 0.1628 - learning_rate: 1.2500e-04
Epoch 60/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s 253ms/step - accuracy: 0.9634 - loss: 0.1413 - val_accuracy: 0.9750 - val_loss: 0.1671 - learning_rate: 1.2500e-04
Epoch 61/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 235ms/step - accuracy: 0.9410 - loss: 0.1847 - val_accuracy: 0.9750 - val_loss: 0.1719 - learning_rate: 1.2500e-04
Epoch 62/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 234ms/step - accuracy: 0.9568 - loss: 0.1359 - val_accuracy: 0.9750 - val_loss: 0.1719 - learning_rate: 1.2500e-04
Epoch 63/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 229ms/step - accuracy: 0.9556 - loss: 0.1789 - val_accuracy: 0.9750 - val_loss: 0.1726 - learning_rate: 1.2500e-04
Epoch 64/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 230ms/step - accuracy: 0.9574 - loss: 0.1556
Epoch 64: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s 244ms/step - accuracy: 0.9574 - loss: 0.1556 - val_accuracy: 0.9750 - val_loss: 0.1699 - learning_rate: 1.2500e-04
Epoch 65/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s 251ms/step - accuracy: 0.9498 - loss: 0.1758 - val_accuracy: 0.9750 - val_loss: 0.1712 - learning_rate: 6.2500e-05
Epoch 66/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 16s 259ms/step - accuracy: 0.9582 - loss: 0.1266 - val_accuracy: 0.9750 - val_loss: 0.1746 - learning_rate: 6.2500e-05
Epoch 67/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 238ms/step - accuracy: 0.9432 - loss: 0.1630 - val_accuracy: 0.9750 - val_loss: 0.1773 - learning_rate: 6.2500e-05
Epoch 68/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 228ms/step - accuracy: 0.9536 - loss: 0.1496 - val_accuracy: 0.9750 - val_loss: 0.1739 - learning_rate: 6.2500e-05
Epoch 69/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 216ms/step - accuracy: 0.9697 - loss: 0.1099
Epoch 69: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 225ms/step - accuracy: 0.9695 - loss: 0.1103 - val_accuracy: 0.9750 - val_loss: 0.1717 - learning_rate: 6.2500e-05
Epoch 70/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 232ms/step - accuracy: 0.9707 - loss: 0.1035 - val_accuracy: 0.9708 - val_loss: 0.1725 - learning_rate: 3.1250e-05
Epoch 71/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 226ms/step - accuracy: 0.9503 - loss: 0.1411 - val_accuracy: 0.9708 - val_loss: 0.1722 - learning_rate: 3.1250e-05
Epoch 72/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 222ms/step - accuracy: 0.9579 - loss: 0.1463 - val_accuracy: 0.9708 - val_loss: 0.1734 - learning_rate: 3.1250e-05
Epoch 73/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 233ms/step - accuracy: 0.9575 - loss: 0.1334 - val_accuracy: 0.9750 - val_loss: 0.1724 - learning_rate: 3.1250e-05
Epoch 74/100
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 220ms/step - accuracy: 0.9476 - loss: 0.1720
Epoch 74: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 228ms/step - accuracy: 0.9476 - loss: 0.1717 - val_accuracy: 0.9750 - val_loss: 0.1719 - learning_rate: 3.1250e-05
Epoch 74: early stopping
Restoring model weights from the end of the best epoch: 59.

============================================================
ğŸ“ˆ FINAL EVALUATION
============================================================

âœ… Training Accuracy: 100.00%
âœ… Test Accuracy: 97.50%

ğŸ’¾ Model saved at: models\voice_model.keras
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`.
ğŸ’¾ Legacy format saved at: models\voice_model.h5

============================================================
âœ¨ Training Complete!
============================================================